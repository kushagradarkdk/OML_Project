# OML_Project
Optimization is one of the core components of machine learning. The essence of most machine learning algorithms is to build an optimization model and learn the parameters in the objective function from the given data. The effectiveness and efficiency of the numerical optimization algorithms dramatically impacts the performance of the machine learning models and machine learning algorithms. From the perspective of the gradient information in optimization, optimization methods can be divided into three categories: ﬁrst-order optimization methods, which are represented by the widely used stochastic gradient methods. High-order optimization methods for example Newton’s method is and heuristic derivative-free optimization methods. 

 

In the project we present a comparative study of different popular first order optimization algorithms which are both, widely in usage and those of which are still in research trails in a deep learning problem setting. In addition we combine a recently developed accelerated strategy with some popular algorithms to test its performance. 
